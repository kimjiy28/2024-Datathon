{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Image Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "class_id = []\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "train_raw = pd.read_csv(\"traffic_train.csv\", index_col = False)\n",
    "\n",
    "def read_image_data(classes):\n",
    "\n",
    "    for i in range(classes):\n",
    "        path = os.path.join(current_dir, \"Train\", str(i)) # acess Train folder\n",
    "        images = os.listdir(path) # get path for all images in the class\n",
    "        for item in images:\n",
    "            image = Image.open(os.path.join(path, item))\n",
    "            image = image.resize((80,80))\n",
    "            image = np.array(image)\n",
    "            train.append(image) # image can be stored as array only\n",
    "            class_id.append(i) # train file is in reverse order, so...\n",
    "\n",
    "read_image_data(43)\n",
    "\n",
    "train = np.array(train)\n",
    "class_id = np.array(class_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, class_id, test_size = 0.2, random_state = 31)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes = 43)\n",
    "y_test = to_categorical(y_test, num_classes = 43) #to be used with the loss = categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31367, 80, 80, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80, 80, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiyeon/anaconda3/envs/datascience/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m3137/3137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m691s\u001b[0m 219ms/step - accuracy: 0.5481 - loss: 2.2480 - val_accuracy: 0.9555 - val_loss: 0.1537\n",
      "Epoch 2/15\n",
      "\u001b[1m3137/3137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m697s\u001b[0m 205ms/step - accuracy: 0.9120 - loss: 0.3107 - val_accuracy: 0.9639 - val_loss: 0.1260\n",
      "Epoch 3/15\n",
      "\u001b[1m3137/3137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m600s\u001b[0m 191ms/step - accuracy: 0.9320 - loss: 0.2370 - val_accuracy: 0.9719 - val_loss: 0.0976\n",
      "Epoch 4/15\n",
      "\u001b[1m3137/3137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m606s\u001b[0m 193ms/step - accuracy: 0.9398 - loss: 0.2092 - val_accuracy: 0.9712 - val_loss: 0.1074\n",
      "Epoch 5/15\n",
      "\u001b[1m3137/3137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 198ms/step - accuracy: 0.9458 - loss: 0.1917 - val_accuracy: 0.9872 - val_loss: 0.0535\n",
      "Epoch 6/15\n",
      "\u001b[1m3137/3137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1803s\u001b[0m 575ms/step - accuracy: 0.9386 - loss: 0.2260 - val_accuracy: 0.9829 - val_loss: 0.0707\n",
      "Epoch 7/15\n",
      "\u001b[1m3137/3137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m596s\u001b[0m 190ms/step - accuracy: 0.9550 - loss: 0.1608 - val_accuracy: 0.9563 - val_loss: 0.1860\n",
      "Epoch 8/15\n",
      "\u001b[1m3137/3137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 174ms/step - accuracy: 0.9514 - loss: 0.1735 - val_accuracy: 0.9832 - val_loss: 0.0748\n",
      "Epoch 9/15\n",
      "\u001b[1m3137/3137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 174ms/step - accuracy: 0.9448 - loss: 0.2126 - val_accuracy: 0.9872 - val_loss: 0.0586\n",
      "Epoch 10/15\n",
      "\u001b[1m3137/3137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m581s\u001b[0m 185ms/step - accuracy: 0.9590 - loss: 0.1527 - val_accuracy: 0.9782 - val_loss: 0.0875\n",
      "Epoch 11/15\n",
      "\u001b[1m3137/3137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m610s\u001b[0m 194ms/step - accuracy: 0.9394 - loss: 0.2384 - val_accuracy: 0.9848 - val_loss: 0.0615\n",
      "Epoch 12/15\n",
      "\u001b[1m3137/3137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m553s\u001b[0m 176ms/step - accuracy: 0.9514 - loss: 0.1893 - val_accuracy: 0.9838 - val_loss: 0.0718\n",
      "Epoch 13/15\n",
      "\u001b[1m3137/3137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m554s\u001b[0m 176ms/step - accuracy: 0.9619 - loss: 0.1356 - val_accuracy: 0.9686 - val_loss: 0.1249\n",
      "Epoch 14/15\n",
      "\u001b[1m3137/3137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m551s\u001b[0m 176ms/step - accuracy: 0.9625 - loss: 0.1379 - val_accuracy: 0.9867 - val_loss: 0.0609\n",
      "Epoch 15/15\n",
      "\u001b[1m3137/3137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 178ms/step - accuracy: 0.9634 - loss: 0.1379 - val_accuracy: 0.9878 - val_loss: 0.0646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x13c96e690>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
    "import keras\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters = 16, kernel_size = (5, 5), activation = \"relu\", input_shape = (80, 80, 3))) # input layer\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation = \"relu\"))\n",
    "model.add(MaxPool2D(pool_size = (2, 2))) # reduce spatial dimention\n",
    "model.add(Dropout(rate = 0.3))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = \"relu\"))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3, 3), activation = \"relu\"))\n",
    "model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(rate = 0.3))\n",
    "model.add(Flatten()) # converts to 1D to match Dense\n",
    "model.add(Dense(units = 128, activation = \"relu\"))\n",
    "model.add(Dropout(rate = 0.3))\n",
    "\n",
    "model.add(Dense(units = 43, activation = \"softmax\")) # output layer\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate = 0.001, weight_decay = 1e-6)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics = [\"accuracy\"]) # soft probability\n",
    "model.fit(X_train, y_train, batch_size = 10, epochs = 15, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12630, 80, 80, 3)\n"
     ]
    }
   ],
   "source": [
    "path_test = os.path.join(current_dir, \"Test\")\n",
    "test_images = os.listdir(path_test)\n",
    "test_images.sort()\n",
    "\n",
    "def get_array(path_list, general_path):\n",
    "\n",
    "    data = []\n",
    "    for item in path_list:\n",
    "        image = Image.open(os.path.join(general_path, item))\n",
    "        image = image.resize((80,80))\n",
    "        image = np.array(image)\n",
    "        data.append(image)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = get_array(test_images, path_test)\n",
    "test = np.array(data)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 98ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_prediction = model.predict(test)\n",
    "output = np.argmax(Y_prediction, axis = 1)\n",
    "\n",
    "test_csv = pd.read_csv(\"traffic_test.csv\", index_col = 0)\n",
    "ids = test_csv.index.to_list()\n",
    "result = pd.DataFrame({\"id\": ids, \"class\": output})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing Result for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"NN_output.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
